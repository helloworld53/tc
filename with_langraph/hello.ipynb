{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba513a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest.py\n",
    "import os, re, glob, json\n",
    "import numpy as np, faiss\n",
    "from openai import OpenAI\n",
    "import streamlit as st \n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "VECTOR_DIR = \"vectorstore\"\n",
    "EMBED_MODEL = \"togethercomputer/m2-bert-80M-32k-retrieval\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "client = OpenAI(api_key=st.secrets[\"OPENAI_API_KEY\"],\n",
    "                base_url=\"https://api.together.xyz/v1\")\n",
    "\n",
    "# ARTICLE_RE = re.compile(r\"(?mi)^\\s*(?:Article|Straipsnis)\\s+(\\d+[A-Za-z\\-]*)\\.?\\s*(.*)$\")\n",
    "ARTICLE_RE = re.compile(r\"(?i)\\b(?:Article|Straipsnis)\\s+(\\d+[A-Za-z\\-]*)\\.?\\s*([^0-9\\n]*)\")\n",
    "\n",
    "def clean_text(t): return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "# def load_docs():\n",
    "#     texts = []\n",
    "#     for p in glob.glob(os.path.join(DATA_DIR, \"**/*\"), recursive=True):\n",
    "#         if p.lower().endswith(\".txt\"):\n",
    "#             texts.append((open(p, encoding=\"utf-8\").read(), {\"source\": p}))\n",
    "#     if not texts:\n",
    "#         raise RuntimeError(\"No docs in ./data\")\n",
    "#     return [(clean_text(t), m) for t,m in texts]\n",
    "def load_docs() -> List:\n",
    "    docs = []\n",
    "    for path in glob.glob(os.path.join(DATA_DIR, \"**/*\"), recursive=True):\n",
    "        if path.lower().endswith(\".pdf\"):\n",
    "            docs.extend(PyPDFLoader(path).load())\n",
    "        elif path.lower().endswith(\".txt\") or path.lower().endswith(\".md\"):\n",
    "            docs.extend(TextLoader(path, encoding=\"utf-8\").load())\n",
    "    if not docs:\n",
    "        raise RuntimeError(\"No documents found in ./data. Add PDFs or txt/md files.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "\n",
    "def split_articles(docs):\n",
    "    chunks = []\n",
    "    for txt, meta in docs:\n",
    "        matches = list(ARTICLE_RE.finditer(txt))\n",
    "        if not matches:\n",
    "            chunks.append({\"text\": txt, \"meta\": {**meta, \"article_id\":\"ALL\",\"chunk_id\":0}})\n",
    "        for i,m in enumerate(matches):\n",
    "            start, end = m.start(), matches[i+1].start() if i+1<len(matches) else len(txt)\n",
    "            art_id, art_title = m.group(1), m.group(2) or f\"Article {m.group(1)}\"\n",
    "            body = txt[start:end]\n",
    "            chunks.append({\"text\": body, \"meta\": {**meta, \"article_id\":art_id, \"article_title\":art_title,\"chunk_id\":0}})\n",
    "    return chunks\n",
    "\n",
    "def embed_texts(texts):\n",
    "    vecs=[]\n",
    "    for i in range(0,len(texts),BATCH_SIZE):\n",
    "        batch=texts[i:i+BATCH_SIZE]\n",
    "        res=client.embeddings.create(model=EMBED_MODEL, input=batch)\n",
    "        vecs.extend([d.embedding for d in res.data])\n",
    "    return vecs\n",
    "\n",
    "# def main():\n",
    "#     os.makedirs(VECTOR_DIR,exist_ok=True)\n",
    "#     docs = load_docs()\n",
    "#     articles = split_articles(docs)\n",
    "#     texts = [c[\"text\"] for c in articles]\n",
    "#     vecs = embed_texts(texts)\n",
    "\n",
    "#     index=faiss.IndexFlatL2(len(vecs[0]))\n",
    "#     index.add(np.array(vecs).astype(\"float32\"))\n",
    "\n",
    "#     faiss.write_index(index, os.path.join(VECTOR_DIR,\"index.faiss\"))\n",
    "#     with open(os.path.join(VECTOR_DIR,\"docstore.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "#         json.dump(articles,f,ensure_ascii=False)\n",
    "#     print(f\"✅ Built FAISS index with {len(articles)} articles\")\n",
    "\n",
    "# if __name__==\"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3e5468e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(VECTOR_DIR,exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m docs \u001b[38;5;241m=\u001b[39m load_docs()\n\u001b[0;32m----> 3\u001b[0m articles \u001b[38;5;241m=\u001b[39m \u001b[43msplit_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m texts \u001b[38;5;241m=\u001b[39m [c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m articles]\n\u001b[1;32m      5\u001b[0m vecs \u001b[38;5;241m=\u001b[39m embed_texts(texts)\n",
      "Cell \u001b[0;32mIn[15], line 45\u001b[0m, in \u001b[0;36msplit_articles\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msplit_articles\u001b[39m(docs):\n\u001b[1;32m     44\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m txt, meta \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m     46\u001b[0m         matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ARTICLE_RE\u001b[38;5;241m.\u001b[39mfinditer(txt))\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matches:\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "os.makedirs(VECTOR_DIR,exist_ok=True)\n",
    "docs = load_docs()\n",
    "articles = split_articles(docs)\n",
    "texts = [c[\"text\"] for c in articles]\n",
    "vecs = embed_texts(texts)\n",
    "\n",
    "index=faiss.IndexFlatL2(len(vecs[0]))\n",
    "index.add(np.array(vecs).astype(\"float32\"))\n",
    "\n",
    "faiss.write_index(index, os.path.join(VECTOR_DIR,\"index.faiss\"))\n",
    "with open(os.path.join(VECTOR_DIR,\"docstore.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(articles,f,ensure_ascii=False)\n",
    "print(f\"✅ Built FAISS index with {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12d51e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'streamlit>=1.35 langchain>=0.2.10 langchain-core>=0.2.10 langchain-openai>=0.1.7 langchain-community>=0.2.10 langgraph>=0.2.21 faiss-cpu>=1.8.0 pydantic>=2.7 pypdf>=4.2.0 tiktoken>=0.7.0',\n",
       "  'meta': {'source': 'requirements.txt', 'article_id': 'ALL', 'chunk_id': 0}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf122338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest.py\n",
    "import os, re, glob, json\n",
    "import numpy as np, faiss\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import streamlit as st \n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ==== Config ====\n",
    "DATA_DIR = \"data\"\n",
    "VECTOR_DIR = \"vectorstore\"\n",
    "EMBED_MODEL = \"togethercomputer/m2-bert-80M-32k-retrieval\"\n",
    "BATCH_SIZE = 32\n",
    "CHUNK_SIZE = 1200   # child chunk size\n",
    "CHUNK_OVERLAP = 150\n",
    "\n",
    "# Together client\n",
    "client = OpenAI(\n",
    "    api_key=st.secrets[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.together.xyz/v1\"\n",
    ")\n",
    "\n",
    "ARTICLE_RE = re.compile(r\"(?i)\\b(?:Article|Straipsnis)\\s+(\\d+[A-Za-z\\-]*)\\.?\\s*([^0-9\\n]*)\")\n",
    "\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "# ==== 1. Load PDFs/TXT into Documents ====\n",
    "def load_docs() -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for path in glob.glob(os.path.join(DATA_DIR, \"**/*\"), recursive=True):\n",
    "        lp = path.lower()\n",
    "        if lp.endswith(\".pdf\"):\n",
    "            docs.extend(PyPDFLoader(path).load())\n",
    "        elif lp.endswith(\".txt\") or lp.endswith(\".md\"):\n",
    "            docs.extend(TextLoader(path, encoding=\"utf-8\").load())\n",
    "    if not docs:\n",
    "        raise RuntimeError(\"❌ No documents found in ./data\")\n",
    "    return docs\n",
    "\n",
    "# ==== 2. Split into articles ====\n",
    "def split_articles(docs: List[Document]):\n",
    "    chunks = []\n",
    "    for d in docs:\n",
    "        txt = clean_text(d.page_content)\n",
    "        meta = d.metadata\n",
    "        matches = list(ARTICLE_RE.finditer(txt))\n",
    "\n",
    "        if not matches:\n",
    "            chunks.append({\n",
    "                \"text\": txt,\n",
    "                \"meta\": {**meta, \"article_id\": \"ALL\", \"article_title\": \"FULL\", \"chunk_id\": 0}\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        for i, m in enumerate(matches):\n",
    "            start = m.start()\n",
    "            end = matches[i+1].start() if i+1 < len(matches) else len(txt)\n",
    "            art_id = m.group(1)\n",
    "            art_title = (m.group(2) or f\"Article {m.group(1)}\").strip()\n",
    "            body = txt[start:end]\n",
    "            chunks.append({\n",
    "                \"text\": body,\n",
    "                \"meta\": {**meta, \"article_id\": art_id, \"article_title\": art_title, \"chunk_id\": 0}\n",
    "            })\n",
    "    return chunks\n",
    "\n",
    "# ==== 3. Split long articles into child chunks ====\n",
    "def split_children(parents, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    children = []\n",
    "    for p in parents:\n",
    "        text = p[\"text\"]\n",
    "        meta = p[\"meta\"]\n",
    "        start, idx = 0, 0\n",
    "        while start < len(text):\n",
    "            end = min(len(text), start + chunk_size)\n",
    "            sub = text[start:end]\n",
    "            child_meta = {**meta, \"chunk_id\": idx}\n",
    "            children.append({\"text\": sub, \"meta\": child_meta})\n",
    "            if end == len(text):\n",
    "                break\n",
    "            start = end - overlap\n",
    "            idx += 1\n",
    "    return children\n",
    "\n",
    "# ==== 4. Embed ====\n",
    "def embed_texts(texts: List[str]):\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        res = client.embeddings.create(model=EMBED_MODEL, input=batch)\n",
    "        vecs.extend([d.embedding for d in res.data])\n",
    "    return vecs\n",
    "\n",
    "# ==== 5. Main ====\n",
    "\n",
    "os.makedirs(VECTOR_DIR, exist_ok=True)\n",
    "docs = load_docs()\n",
    "articles = split_articles(docs)\n",
    "children = split_children(articles, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "# texts = [c[\"text\"] for c in children]\n",
    "# vecs = embed_texts(texts)\n",
    "\n",
    "# index = faiss.IndexFlatL2(len(vecs[0]))\n",
    "# index.add(np.array(vecs).astype(\"float32\"))\n",
    "\n",
    "# faiss.write_index(index, os.path.join(VECTOR_DIR, \"index.faiss\"))\n",
    "# with open(os.path.join(VECTOR_DIR, \"docstore.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(children, f, ensure_ascii=False)\n",
    "\n",
    "# print(f\"✅ Built FAISS index with {len(children)} child chunks from {len(articles)} articles\")\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61e7daeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 35\u001b[0m\n\u001b[1;32m     28\u001b[0m         chunks\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: body,\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: art_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle_title\u001b[39m\u001b[38;5;124m\"\u001b[39m: art_title, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m     31\u001b[0m         })\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m---> 35\u001b[0m articles \u001b[38;5;241m=\u001b[39m \u001b[43msplit_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43md1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m, in \u001b[0;36msplit_articles\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msplit_articles\u001b[39m(docs):\n\u001b[0;32m---> 11\u001b[0m     full_text \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ARTICLE_RE\u001b[38;5;241m.\u001b[39mfinditer(full_text))\n\u001b[1;32m     13\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m, in \u001b[0;36mmerge_docs\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge_docs\u001b[39m(docs):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([clean_page_text(d\u001b[38;5;241m.\u001b[39mpage_content) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs])\n",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge_docs\u001b[39m(docs):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([clean_page_text(\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "def clean_page_text(text: str) -> str:\n",
    "    # Remove URLs, timestamps, “Printed from” etc\n",
    "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\d+/\\d+/\\d+,\\s*\\d+:\\d+\\s*(AM|PM)?\", \"\", text)\n",
    "    text = re.sub(r\"Atspausdinta.*?lt\", \"\", text, flags=re.I)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "def merge_docs(docs):\n",
    "    return \"\\n\".join([clean_page_text(d.page_content) for d in docs])\n",
    "d1 = merge_docs(docs)\n",
    "def split_articles(docs):\n",
    "    full_text = merge_docs(docs)\n",
    "    matches = list(ARTICLE_RE.finditer(full_text))\n",
    "    chunks = []\n",
    "\n",
    "    if not matches:\n",
    "        chunks.append({\n",
    "            \"text\": full_text,\n",
    "            \"meta\": {\"article_id\": \"ALL\", \"article_title\": \"FULL\", \"chunk_id\": 0}\n",
    "        })\n",
    "        return chunks\n",
    "\n",
    "    for i, m in enumerate(matches):\n",
    "        start = m.start()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(full_text)\n",
    "        art_id = m.group(1)\n",
    "        art_title = (m.group(2) or f\"Article {art_id}\").strip()\n",
    "        body = full_text[start:end]\n",
    "        chunks.append({\n",
    "            \"text\": body,\n",
    "            \"meta\": {\"article_id\": art_id, \"article_title\": art_title, \"chunk_id\": 0}\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "\n",
    "articles = split_articles(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adc1cf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Article: ALL FULL\n",
      "Page: 31 Dist: 0.07813991\n",
      "idity of the travel document and, after the foreigner produces a new valid travel document, may be re-personalised for the remaining duration of validity of the temporary residence permit. 5. A foreigner being in possession of a temporary residence permit must, in the event of changes in the circums ...\n",
      "----\n",
      "Article: ALL FULL\n",
      "Page: 31 Dist: 0.082836345\n",
      "descent 1. A foreigner of Lithuanian descent may be issued a temporary residence permit if he provides proof of his Lithuanian descent. 2. A foreigner of Lithuanian descent shall be issued a temporary residence permit for five years. 8/23/25, 12:33 PM Republic of Lithuania Law on the Legal Status of ...\n",
      "----\n",
      "Article: ALL FULL\n",
      "Page: 6 Dist: 0.09762402\n",
      " objective reasons is unable to obtain travel documents from his country of origin, where such a document grants him the right to leave and return to the Republic of Lithuania for the duration of validity of the document. 291. Resettlement/relocation of foreigners to the territory of the Republic of ...\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np, faiss\n",
    "from openai import OpenAI\n",
    "\n",
    "VECTOR_DIR = \"vectorstore\"\n",
    "EMBED_MODEL = \"togethercomputer/m2-bert-80M-32k-retrieval\"\n",
    "\n",
    "index = faiss.read_index(f\"{VECTOR_DIR}/index.faiss\")\n",
    "docstore = json.load(open(f\"{VECTOR_DIR}/docstore.json\", encoding=\"utf-8\"))\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"], base_url=\"https://api.together.xyz/v1\")\n",
    "\n",
    "query = \"What are the rules for residence permits in Lithuania?\"\n",
    "q_vec = client.embeddings.create(model=EMBED_MODEL, input=query).data[0].embedding\n",
    "\n",
    "D, I = index.search(np.array([q_vec], dtype=\"float32\"), k=3)\n",
    "for idx, dist in zip(I[0], D[0]):\n",
    "    chunk = docstore[idx]\n",
    "    print(\"----\")\n",
    "    print(\"Article:\", chunk[\"meta\"].get(\"article_id\"), chunk[\"meta\"].get(\"article_title\"))\n",
    "    print(\"Page:\", chunk[\"meta\"].get(\"page\"), \"Dist:\", dist)\n",
    "    print(chunk[\"text\"][:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04adc1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Article: ALL FULL\n",
      "Page: 31 Dist: 0.07813991\n",
      "idity of the travel document and, after the foreigner produces a new valid travel document, may be re-personalised for the remaining duration of validity of the temporary residence permit. 5. A foreigner being in possession of a temporary residence permit must, in the event of changes in the circumstances due to which the permit has been issued, obtain a new temporary residence permit. 6. A family member who enters the Republic of Lithuania for residence accompanying a foreigner who applies for the issue of a temporary residence permit on the ground laid down in point 41, 42, 51, 13 or 16 of paragraph 1 of this Article or on the ground laid down in point 14 of paragraph 1 of this Article and fulfils the condition set out in Article 43(6)(3) of this Law or on the grounds laid down in points 1 to 22 of Article 45(1) of this Law or who accompanies a foreigner referred to in point 4 or 5 of Article 43(6) of this Law may apply for the issue of a temporary residence permit together with the said foreigner and a temporary residence permit shall be issued to him for the same period as to the foreigner. An application of a family member who enters the Republic of Lithuania for residence acc ...\n",
      "----\n",
      "Article: ALL FULL\n",
      "Page: 31 Dist: 0.082836345\n",
      "descent 1. A foreigner of Lithuanian descent may be issued a temporary residence permit if he provides proof of his Lithuanian descent. 2. A foreigner of Lithuanian descent shall be issued a temporary residence permit for five years. 8/23/25, 12:33 PM Republic of Lithuania Law on the Legal Status of Foreigners file:///C:/Users/karan/Downloads/Republic of Lithuania Law on the Legal Status of Foreigners.mhtml 32/123 ...\n",
      "----\n",
      "Article: ALL FULL\n",
      "Page: 6 Dist: 0.09762402\n",
      " objective reasons is unable to obtain travel documents from his country of origin, where such a document grants him the right to leave and return to the Republic of Lithuania for the duration of validity of the document. 291. Resettlement/relocation of foreigners to the territory of the Republic of Lithuania means the transfer of foreigners meeting the criteria established by this Law for the granting of asylum from the territory of an EU Member State or a third country to the territory of the Republic of Lithuania at European Union level or under bilateral agreements with the EU Member States or third countries. 292. Repealed as of 20 September 2016. 30. Foreigner’s registration certificate means a document confirming the right of a foreigner to remain in the territory of the Republic of Lithuania and the right to take up employment (when such right is acquired). 31. Detention of a foreigner shall mean the temporary accommodation of a foreigner at the State Border Guard Service under the Ministry of the Interior of the Republic of Lithuania (hereinafter: the ‘State Border Guard Service’), while restricting his freedom of movement on the grounds and within the time limits laid dow ...\n"
     ]
    }
   ],
   "source": [
    "for idx, dist in zip(I[0], D[0]):\n",
    "    chunk = docstore[idx]\n",
    "    print(\"----\")\n",
    "    print(\"Article:\", chunk[\"meta\"].get(\"article_id\"), chunk[\"meta\"].get(\"article_title\"))\n",
    "    print(\"Page:\", chunk[\"meta\"].get(\"page\"), \"Dist:\", dist)\n",
    "    print(chunk[\"text\"], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef387256",
   "metadata": {},
   "source": [
    "MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42145423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/inspect.py:632: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  def _is_wrapper(f):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FastMCP' object has no attribute 'run_async'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mrun(mcp\u001b[38;5;241m.\u001b[39mrun_async())\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m(mcp\u001b[38;5;241m.\u001b[39mrun_async())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FastMCP' object has no attribute 'run_async'"
     ]
    }
   ],
   "source": [
    "# Looks like your MCP version doesn't export `tool` directly.\n",
    "# In the latest `fastmcp`, tools are declared via decorators inside `FastMCP`.\n",
    "# Let's fix rag_mcp.py accordingly.\n",
    "import streamlit as st  \n",
    "import os, json, faiss, numpy as np\n",
    "from openai import OpenAI\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "VECTOR_DIR = \"vectorstore\"\n",
    "INDEX_PATH = os.path.join(VECTOR_DIR, \"index.faiss\")\n",
    "DOCSTORE_PATH = os.path.join(VECTOR_DIR, \"docstore.json\")\n",
    "EMBED_MODEL = \"togethercomputer/m2-bert-80M-32k-retrieval\"\n",
    "\n",
    "# Load FAISS + docstore\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "docstore = json.load(open(DOCSTORE_PATH, encoding=\"utf-8\"))\n",
    "\n",
    "# Together client\n",
    "client = OpenAI(\n",
    "    api_key=st.secrets[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.together.xyz/v1\"\n",
    ")\n",
    "\n",
    "# Define MCP server\n",
    "mcp = FastMCP(\"rag-mcp\")\n",
    "\n",
    "@mcp.tool()\n",
    "def embed_and_search(query: str, k: int = 5):\n",
    "    \"\"\"Embed a query and return top-k FAISS matches (IDs + distances).\"\"\"\n",
    "    q_vec = client.embeddings.create(model=EMBED_MODEL, input=query).data[0].embedding\n",
    "    D, I = index.search(np.array([q_vec], dtype=\"float32\"), k)\n",
    "    return {\"hits\": [{\"id\": int(i), \"dist\": float(d)} for i, d in zip(I[0], D[0])]} \n",
    "\n",
    "@mcp.tool()\n",
    "def get_context(doc_ids: list, window: int = 1, max_chars: int = 5000):\n",
    "    \"\"\"Return merged context for given doc IDs, expanding neighbors in same article.\"\"\"\n",
    "    expanded = []\n",
    "    for i in doc_ids:\n",
    "        base = docstore[int(i)]\n",
    "        art_id = base[\"meta\"].get(\"article_id\")\n",
    "        # expand all chunks from the same article\n",
    "        article_chunks = [c for c in docstore if c[\"meta\"].get(\"article_id\") == art_id]\n",
    "        expanded.extend(article_chunks)\n",
    "\n",
    "    # Merge into one big context (truncate if too long)\n",
    "    text = \"\\n\".join([c[\"text\"] for c in expanded])[:max_chars]\n",
    "    return {\"contexts\": [text]}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    asyncio.run(mcp.run_async())\n",
    "    # await(mcp.run_async())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a59f1c0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object _AsyncGeneratorContextManager can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# await main()\u001b[39;00m\n",
      "File \u001b[0;32m/home/helloworld/outetts/.venv/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/home/helloworld/outetts/.venv/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[3], line 53\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m---> 53\u001b[0m     session, tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_mcp_tools()\n\u001b[1;32m     54\u001b[0m     graph \u001b[38;5;241m=\u001b[39m build_graph(tools)\n\u001b[1;32m     55\u001b[0m     state \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease greet Alice and add 2 + 3.\u001b[39m\u001b[38;5;124m\"\u001b[39m)]}\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mget_mcp_tools\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_mcp_tools\u001b[39m():\n\u001b[1;32m     21\u001b[0m     server \u001b[38;5;241m=\u001b[39m StdioServerParameters(command\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple_mcp.py\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 22\u001b[0m     read, write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m stdio_client(server)\n\u001b[1;32m     23\u001b[0m     session \u001b[38;5;241m=\u001b[39m ClientSession(read, write)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m session\u001b[38;5;241m.\u001b[39minitialize()\n",
      "\u001b[0;31mTypeError\u001b[0m: object _AsyncGeneratorContextManager can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "# !pip install nest_asyncio\n",
    "import nest_asyncio, asyncio\n",
    "nest_asyncio.apply()\n",
    "# simple_graph.py\n",
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "from mcp.client.stdio import stdio_client\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "\n",
    "# ---- State ----\n",
    "from typing import TypedDict, List\n",
    "class State(TypedDict):\n",
    "    messages: List\n",
    "\n",
    "# ---- Connect to MCP server ----\n",
    "async def get_mcp_tools():\n",
    "    server = StdioServerParameters(command=\"python\", args=[\"simple_mcp.py\"])\n",
    "    read, write = await stdio_client(server)\n",
    "    session = ClientSession(read, write)\n",
    "    await session.initialize()\n",
    "\n",
    "    tools = await session.list_tools()\n",
    "    return session, tools.tools\n",
    "\n",
    "# ---- Build agent ----\n",
    "def build_graph(tools):\n",
    "    g = StateGraph(State)\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"meta-llama/Llama-3-8b-chat-hf\",\n",
    "        api_key=\"your_together_api_key\",\n",
    "        base_url=\"https://api.together.xyz/v1\",\n",
    "        temperature=0\n",
    "    ).bind_tools(tools)\n",
    "\n",
    "    def step(state: State):\n",
    "        if not state[\"messages\"]:\n",
    "            return state\n",
    "        ai = llm.invoke(state[\"messages\"])\n",
    "        state[\"messages\"].append(ai)\n",
    "        return state\n",
    "\n",
    "    g.add_node(\"step\", step)\n",
    "    g.set_entry_point(\"step\")\n",
    "    g.add_edge(\"step\", END)\n",
    "    return g.compile()\n",
    "\n",
    "# ---- Run ----\n",
    "async def main():\n",
    "    session, tools = await get_mcp_tools()\n",
    "    graph = build_graph(tools)\n",
    "    state = {\"messages\": [HumanMessage(content=\"Please greet Alice and add 2 + 3.\")]}\n",
    "    out = graph.invoke(state)\n",
    "    print(out[\"messages\"][-1].content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "    # await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
